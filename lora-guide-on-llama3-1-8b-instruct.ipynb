{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":81881,"sourceType":"modelInstanceVersion","modelInstanceId":68809,"modelId":91102}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/kajuyerim/lora-guide-on-llama3-1-8b-instruct?scriptVersionId=193274184\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# LoRA Guide About Sentimental Analysis on Financial Domain Using Llama3.1 8b-instruct\n---\n\n<font size=\"5\">**What is LoRA?**</font> \n* <font size=\"3\"> Low Rank Adaptation: Freezing the pretrained model weights and injecting trainable rank decomposition matrices into each layer of the Transformer architecture.</font> \n* <font size=\"3\"> Main purpose is to lower the dimensions of the matrix. </font> \n* <font size=\"3\"> Instead of updating all weights of a model, we only update the injected low rank matrices.</font> \n\n<font size=\"5\">**Why Use LoRA?**</font> \n* <font size=\"3\"> Greatly reducing the number of trainable parameters (up to 10000 times).\n* <font size=\"3\"> Reducing the GPU memory requirement (up to 3 times).\n    \n<font size=\"5\">**References**</font> \n* <font size=\"3\"> https://www.datacamp.com/tutorial/fine-tuning-llama-3-1  \n* <font size=\"3\"> https://towardsdatascience.com/implementing-lora-from-scratch-20f838b046f1  \n* <font size=\"3\"> https://lightning.ai/pages/community/tutorial/lora-llm/  \n    \n\n---\n<font size=\"4\">**! This guide is based on the LoRA paper (LoRA: Low-Rank Adaptation for Neural Networks)** (https://arxiv.org/abs/2106.09685#).</font>   \n<font size=\"4\">**! Any mistake cbetween this guide and the paper is due to my interpretation on the material.**</font>  \n\n---\n","metadata":{}},{"cell_type":"code","source":"%%capture\n%pip install -U bitsandbytes\n%pip install -U transformers\n%pip install -U accelerate\n%pip install -U trl\n%pip install -U peft\n%pip install -U wandb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-20T06:16:14.604278Z","iopub.execute_input":"2024-08-20T06:16:14.604559Z","iopub.status.idle":"2024-08-20T06:17:35.035864Z","shell.execute_reply.started":"2024-08-20T06:16:14.604532Z","shell.execute_reply":"2024-08-20T06:17:35.034458Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Initiating Hugging Face & Wandb Login\n<font size=\"3\">We need to log in to Hugging Face for uploading and using the LLama3.1 model, and Wandb for training performance analysis.  \n<font size=\"3\">Insert your own token to HUGGINGFACE_TOKEN part (Use secrets).  \n<font size=\"3\">Insert your own token to WANDB_TOKEN part (Use secrets).\n    \n### Wand Library for Fine-Tuning Progress Tracking\n\nThe `Wand` library is a Python interface for [ImageMagick](https://imagemagick.org/), a powerful image manipulation tool. While Wand is typically used for image processing, it can be creatively repurposed to visualize and track the progress of machine learning model fine-tuning.\n\n#### Use Case: Visualizing Fine-Tuning Progress\n\nDuring fine-tuning, visualizing metrics like loss curves, accuracy over epochs, or confusion matrices can be helpful. Wand can assist in creating these visualizations and saving them as images for each epoch or checkpoint, allowing you to track progress over time.\n\n#### Key Benefits\n\n- **Automated Progress Images**: Automatically generate images that represent the state of your model at each fine-tuning stage.\n- **Custom Visualizations**: Draw graphs, add text annotations, and combine images to create comprehensive progress reports.\n- **Format Flexibility**: Save your visualizations in various formats (e.g., PNG, JPEG), making them easy to share or include in reports.\n\n","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nimport wandb\n\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nwb_token = user_secrets.get_secret(\"WANDB_TOKEN\")\n\nlogin(token = hf_token)\nwandb.login(key=wb_token)\nrun = wandb.init(\n    project='Fine tuning LLama3.1 8b for Sentimental Analysis on Financial Domain', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T06:17:35.037563Z","iopub.execute_input":"2024-08-20T06:17:35.037901Z","iopub.status.idle":"2024-08-20T06:17:56.347335Z","shell.execute_reply.started":"2024-08-20T06:17:35.03787Z","shell.execute_reply":"2024-08-20T06:17:56.346274Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkajuyerim\u001b[0m (\u001b[33mkajuyerim-bo-azi-i-niversitesi\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240820_061739-3wqscyc9</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/kajuyerim-bo-azi-i-niversitesi/Fine%20tuning%20LLama3.1%208b%20for%20Sentimental%20Analysis%20on%20Financial%20Domain/runs/3wqscyc9' target=\"_blank\">dutiful-hill-8</a></strong> to <a href='https://wandb.ai/kajuyerim-bo-azi-i-niversitesi/Fine%20tuning%20LLama3.1%208b%20for%20Sentimental%20Analysis%20on%20Financial%20Domain' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/kajuyerim-bo-azi-i-niversitesi/Fine%20tuning%20LLama3.1%208b%20for%20Sentimental%20Analysis%20on%20Financial%20Domain' target=\"_blank\">https://wandb.ai/kajuyerim-bo-azi-i-niversitesi/Fine%20tuning%20LLama3.1%208b%20for%20Sentimental%20Analysis%20on%20Financial%20Domain</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/kajuyerim-bo-azi-i-niversitesi/Fine%20tuning%20LLama3.1%208b%20for%20Sentimental%20Analysis%20on%20Financial%20Domain/runs/3wqscyc9' target=\"_blank\">https://wandb.ai/kajuyerim-bo-azi-i-niversitesi/Fine%20tuning%20LLama3.1%208b%20for%20Sentimental%20Analysis%20on%20Financial%20Domain/runs/3wqscyc9</a>"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\nimport os\nfrom tqdm import tqdm\nimport bitsandbytes as bnb\nimport torch\nimport torch.nn as nn\nimport transformers\nfrom datasets import Dataset\nfrom trl import SFTTrainer\nfrom trl import setup_chat_format\nfrom transformers import (AutoModelForCausalLM, \n                          AutoTokenizer, \n                          BitsAndBytesConfig, \n                          TrainingArguments, \n                          pipeline, \n                          logging)\nfrom sklearn.metrics import (accuracy_score, \n                             classification_report, \n                             confusion_matrix)\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-08-20T06:17:56.350786Z","iopub.execute_input":"2024-08-20T06:17:56.351185Z","iopub.status.idle":"2024-08-20T06:18:04.59412Z","shell.execute_reply.started":"2024-08-20T06:17:56.351148Z","shell.execute_reply":"2024-08-20T06:18:04.593222Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-08-20 06:18:00.500843: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-20 06:18:00.500907: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-20 06:18:00.502745: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Loading and Explaining Dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import load_dataset\n\n# Load and shuffle the dataset (seed for reproducing)\ndataset = load_dataset(\"takala/financial_phrasebank\", \"sentences_allagree\", split='train')\nshuffled_dataset = dataset.shuffle(seed=32)\n\ndf = shuffled_dataset.to_pandas()\n\n# Map sentiment labels to text (optional)\nlabel_mapping = {0: 'negative', 1: 'neutral', 2: 'positive'}\ndf['label'] = df['label'].map(label_mapping)\n\n# Save the dataset to a CSV file\ndf.to_csv('financial_phrasebank.csv', index=False)\n\n# Display the updated DataFrame\nprint(df.head())\ndf.label.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-08-20T06:18:04.595246Z","iopub.execute_input":"2024-08-20T06:18:04.595829Z","iopub.status.idle":"2024-08-20T06:18:06.102866Z","shell.execute_reply.started":"2024-08-20T06:18:04.595802Z","shell.execute_reply":"2024-08-20T06:18:06.101762Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"                                            sentence     label\n0             The value of the order is USD 2.2 mn .   neutral\n1  YIT lodged counter claims against Neste Oil to...  negative\n2  Rohwedder Group is an automotive supplies , te...   neutral\n3  Nokia was up 0.12 pct to 16.70 eur after kicki...  positive\n4  The Group owns and operates a fleet of more th...   neutral\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"label\nneutral     1391\npositive     570\nnegative     303\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\n\n# Split the DataFrame\ntrain_size = 0.8\neval_size = 0.1\n\n# Calculate sizes\ntrain_end = int(train_size * len(df))\neval_end = train_end + int(eval_size * len(df))\n\n# Split the data\nX_train = df[:train_end]\nX_eval = df[train_end:eval_end]\nX_test = df[eval_end:]\n\n# Keep a copy of the test labels before generating prompts\ny_true = X_test['label'].values \n\n# Define the prompt generation functions\ndef generate_prompt(data_point):\n    return f\"\"\"\n            Classify the financial text as Positive, Negative, or Neutral.\ntext: {data_point['sentence']}\nlabel: {data_point['label']}\"\"\".strip()\n\ndef generate_test_prompt(data_point):\n    return f\"\"\"\n            Classify the financial text as Positive, Negative, or Neutral.\ntext: {data_point['sentence']}\nlabel: \"\"\".strip()\n\n# Generate prompts for training and evaluation data\nX_train.loc[:,'text'] = X_train.apply(generate_prompt, axis=1)\nX_eval.loc[:,'text'] = X_eval.apply(generate_prompt, axis=1)\n\n# Generate test prompts (without labels)\nX_test = pd.DataFrame(X_test.apply(generate_test_prompt, axis=1), columns=[\"text\"])\n\n## If you want to see all of the text -> Use these:\n# pd.set_option('display.max_colwidth', None)  # Set max column width to None to avoid truncation\n# pd.set_option('display.max_columns', None)   # Show all columns\n\n# Random 5 rows\nprint(X_train.head(5))\n\n# Now you have the correct y_true for evaluation\n# X_train.label.value_counts() can be used to check label distribution in the training set\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T06:18:06.104676Z","iopub.execute_input":"2024-08-20T06:18:06.105075Z","iopub.status.idle":"2024-08-20T06:18:06.172693Z","shell.execute_reply.started":"2024-08-20T06:18:06.105027Z","shell.execute_reply":"2024-08-20T06:18:06.171376Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"                                            sentence     label  \\\n0             The value of the order is USD 2.2 mn .   neutral   \n1  YIT lodged counter claims against Neste Oil to...  negative   \n2  Rohwedder Group is an automotive supplies , te...   neutral   \n3  Nokia was up 0.12 pct to 16.70 eur after kicki...  positive   \n4  The Group owns and operates a fleet of more th...   neutral   \n\n                                                text  \n0  Classify the financial text as Positive, Negat...  \n1  Classify the financial text as Positive, Negat...  \n2  Classify the financial text as Positive, Negat...  \n3  Classify the financial text as Positive, Negat...  \n4  Classify the financial text as Positive, Negat...  \n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_282/168374931.py:33: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  X_train.loc[:,'text'] = X_train.apply(generate_prompt, axis=1)\n/tmp/ipykernel_282/168374931.py:34: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  X_eval.loc[:,'text'] = X_eval.apply(generate_prompt, axis=1)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Convert to datasets\ntrain_data = Dataset.from_pandas(X_train[[\"text\"]])\neval_data = Dataset.from_pandas(X_eval[[\"text\"]])\n\ntrain_data.shuffle(seed=42)['text'][3:6]","metadata":{"execution":{"iopub.status.busy":"2024-08-20T06:18:06.174215Z","iopub.execute_input":"2024-08-20T06:18:06.17463Z","iopub.status.idle":"2024-08-20T06:18:06.220936Z","shell.execute_reply.started":"2024-08-20T06:18:06.174591Z","shell.execute_reply":"2024-08-20T06:18:06.219761Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"['Classify the financial text as Positive, Negative, or Neutral.\\ntext: In the sinter plant , limestone and coke breeze are mixed with the iron ore concentrate and sintered into lump form or sinter for use in the blast furnaces as a raw material for iron-making .\\nlabel: neutral',\n 'Classify the financial text as Positive, Negative, or Neutral.\\ntext: The Oxyview Pulse Oximeter is a common device to check patient blood-oxygen saturation level and pulse rate .\\nlabel: neutral',\n \"Classify the financial text as Positive, Negative, or Neutral.\\ntext: Theodosopoulos said Tellabs could be of value to Nokia Siemens or Nortel given its `` leading supply status '' with Verizon , along with high-growth products .\\nlabel: positive\"]"},"metadata":{}}]},{"cell_type":"markdown","source":"# Loading the Model LLama3.1 8b-instruct\n\n### What is Quantization?\n\nQuantization reduces the precision of a model's weights and activations, typically from 32-bit floats to lower-bit integers, to make the model smaller and faster.\n\n### How Quantization Works\n\n1. **Scaling Values**: \n   - Example: A 32-bit floating-point value like `0.123456` might be scaled to fit within the range of an 8-bit integer, such as `123` in a range of `0-255`.\n\n2. **Reducing Precision**: \n   - Example: The scaled value `123.456` could be rounded to `123`, fitting within an 8-bit integer, reducing memory and computation requirements.\n\nQuantization optimizes models for deployment on resource-constrained devices, with a potential trade-off in accuracy.\n\n### Reasons for Using Quantization in Fine-Tuning\n\n#### Pros:\n1. **Memory Efficiency**: Reduces model size, enabling fine-tuning on devices with limited memory.\n2. **Faster Computation**: Speeds up training and inference by using lower precision.\n3. **Resource Optimization**: Allows fine-tuning on less powerful, more affordable hardware.\n\n#### Cons:\n1. **Loss of Precision**: Can decrease model accuracy due to reduced precision.\n2. **Increased Complexity**: Adds complexity to setup and may require more careful tuning.\n3. **Potential Instability**: May cause instability during training, especially with small datasets.\n\n### Settings\n1. **`load_in_4bit=True`**:\n   - Enables 4-bit quantization, reducing memory usage.\n\n2. **`bnb_4bit_use_double_quant=False`**:\n   - Disables double quantization to simplify computations.\n\n3. **`bnb_4bit_quant_type=\"nf4\"`**:\n   - Uses the \"nf4\" format, optimized for balancing precision and performance.\n\n4. **`bnb_4bit_compute_dtype=\"float16\"`**:\n   - Computes in 16-bit float, balancing precision and efficiency.\n","metadata":{}},{"cell_type":"code","source":"base_model_name = \"/kaggle/input/llama-3.1/transformers/8b-instruct/1\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=False,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=\"float16\",\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model_name,\n    device_map=\"auto\",\n    torch_dtype=\"float16\",\n    quantization_config=bnb_config, \n)\n\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\n\ntokenizer.pad_token_id = tokenizer.eos_token_id","metadata":{"execution":{"iopub.status.busy":"2024-08-20T06:18:06.222649Z","iopub.execute_input":"2024-08-20T06:18:06.223057Z","iopub.status.idle":"2024-08-20T06:18:22.369655Z","shell.execute_reply.started":"2024-08-20T06:18:06.223019Z","shell.execute_reply":"2024-08-20T06:18:22.368351Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2029d7579f74fb288ceb9a1ea7eb8ae"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Evaluating Before Fine-Tuning\n\n### Why Evaluating a Model Before Fine-Tuning is Important\n\n#### 1. Establishing a Baseline Performance\n- **Purpose:** Understand the model's initial performance on your task.\n- **Benefit:** Provides a reference point to measure the impact of fine-tuning.\n\n#### 2. Understanding Model's Initial Capabilities\n- **Purpose:** Assess the model’s existing knowledge relevant to your task.\n- **Benefit:** Helps determine if fine-tuning is necessary or if the model is already performing well.\n\n#### 3. Identifying the Need for Fine-Tuning\n- **Purpose:** Evaluate if the model needs fine-tuning.\n- **Benefit:** Saves time and resources by avoiding unnecessary fine-tuning.\n\n#### 4. Diagnosing Model Weaknesses\n- **Purpose:** Identify areas where the model struggles.\n- **Benefit:** Guides targeted fine-tuning to address specific issues.\n\n#### 5. Guiding Fine-Tuning Strategy\n- **Purpose:** Inform decisions on which layers or parts of the model to fine-tune.\n- **Benefit:** Ensures efficient and effective fine-tuning.\n\n#### 6. Ensuring Reproducibility\n- **Purpose:** Document performance to track changes and improvements.\n- **Benefit:** Maintains a clear record of the model’s development.\n","metadata":{}},{"cell_type":"code","source":"def predict(test, model, tokenizer):\n    y_pred = []\n    categories = [\"positive\", \"negative\", \"neutral\"]\n    \n    for i in tqdm(range(len(test))):\n        prompt = test.iloc[i][\"text\"]\n        pipe = pipeline(task=\"text-generation\", \n                        model=model, \n                        tokenizer=tokenizer, \n                        max_new_tokens=2, \n                        temperature=0.1)\n        \n        result = pipe(prompt)\n        answer = result[0]['generated_text'].split(\"label:\")[-1].strip()\n        \n        # Determine the predicted category\n        for category in categories:\n            if category.lower() in answer.lower():\n                y_pred.append(category)\n                break\n        else:\n            y_pred.append(\"none\")\n    \n    return y_pred\n\ny_pred = predict(X_test, model, tokenizer)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T06:18:22.371306Z","iopub.execute_input":"2024-08-20T06:18:22.371804Z","iopub.status.idle":"2024-08-20T06:19:42.904127Z","shell.execute_reply.started":"2024-08-20T06:18:22.371761Z","shell.execute_reply":"2024-08-20T06:19:42.903011Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"100%|██████████| 227/227 [01:20<00:00,  2.82it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"def evaluate(y_true, y_pred):\n    labels = [\"positive\", \"negative\", \"neutral\"]\n    mapping = {label: idx for idx, label in enumerate(labels)}\n    \n    def map_func(x):\n        return mapping.get(x, -1)  # Map to -1 if not found, but should not occur with correct data\n    \n    y_true_mapped = np.vectorize(map_func)(y_true)\n    y_pred_mapped = np.vectorize(map_func)(y_pred)\n    \n    # Filter out any -1 values which represent missing or incorrect labels\n    valid_indices = (y_true_mapped != -1) & (y_pred_mapped != -1)\n    y_true_mapped = y_true_mapped[valid_indices]\n    y_pred_mapped = y_pred_mapped[valid_indices]\n    \n    # Check if there are any valid labels left\n    if len(y_true_mapped) == 0 or len(y_pred_mapped) == 0:\n        print(\"No valid labels found in y_true or y_pred.\")\n        return\n    \n    # Calculate accuracy\n    accuracy = accuracy_score(y_true=y_true_mapped, y_pred=y_pred_mapped)\n    print(f'Accuracy: {accuracy:.3f}')\n    \n    # Generate accuracy report\n    unique_labels = set(y_true_mapped) \n    \n    for label in unique_labels:\n        label_indices = [i for i in range(len(y_true_mapped)) if y_true_mapped[i] == label]\n        label_y_true = [y_true_mapped[i] for i in label_indices]\n        label_y_pred = [y_pred_mapped[i] for i in label_indices]\n        label_accuracy = accuracy_score(label_y_true, label_y_pred)\n        print(f'Accuracy for label {labels[label]}: {label_accuracy:.3f}')\n        \n    # Generate classification report\n    class_report = classification_report(y_true=y_true_mapped, y_pred=y_pred_mapped, target_names=labels, labels=list(range(len(labels))))\n    print('\\nClassification Report:')\n    print(class_report)\n    \n    # Generate confusion matrix\n    present_labels = list(unique_labels)\n    if len(present_labels) == 0:\n        print(\"No labels present for confusion matrix.\")\n    else:\n        conf_matrix = confusion_matrix(y_true=y_true_mapped, y_pred=y_pred_mapped, labels=present_labels)\n        print('\\nConfusion Matrix:')\n        print(conf_matrix)\n\n# Generate predictions using the updated X_test (which now contains only text prompts)\ny_pred = predict(X_test, model, tokenizer)\n# Check the contents of y_true\nprint(\"y_true (first 10 elements):\", y_true[:10])\nprint(\"Unique values in y_true:\", set(y_true))\n\n# Check the contents of y_pred\nprint(\"y_pred (first 10 elements):\", y_pred[:10])\nprint(\"Unique values in y_pred:\", set(y_pred))\n\n# Evaluate the predictions against the true labels stored in y_true\nevaluate(y_true, y_pred)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T06:19:42.905626Z","iopub.execute_input":"2024-08-20T06:19:42.90626Z","iopub.status.idle":"2024-08-20T06:21:03.106338Z","shell.execute_reply.started":"2024-08-20T06:19:42.906221Z","shell.execute_reply":"2024-08-20T06:21:03.105265Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"100%|██████████| 227/227 [01:20<00:00,  2.83it/s]","output_type":"stream"},{"name":"stdout","text":"y_true (first 10 elements): ['positive' 'negative' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n 'neutral' 'positive' 'neutral']\nUnique values in y_true: {'negative', 'neutral', 'positive'}\ny_pred (first 10 elements): ['positive', 'negative', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral']\nUnique values in y_pred: {'negative', 'neutral', 'positive'}\nAccuracy: 0.837\nAccuracy for label positive: 0.924\nAccuracy for label negative: 1.000\nAccuracy for label neutral: 0.765\n\nClassification Report:\n              precision    recall  f1-score   support\n\n    positive       0.71      0.92      0.80        66\n    negative       0.76      1.00      0.86        25\n     neutral       0.96      0.76      0.85       136\n\n    accuracy                           0.84       227\n   macro avg       0.81      0.90      0.84       227\nweighted avg       0.87      0.84      0.84       227\n\n\nConfusion Matrix:\n[[ 61   1   4]\n [  0  25   0]\n [ 25   7 104]]\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Explanation of the Scores and Measurement Criteria\n\n#### **Overall Accuracy: 0.837**\n- **What it means:** This represents the proportion of total correct predictions made by the model across all classes. In this case, the model correctly predicted approximately 83.7% of the instances.\n- **Calculation:**  \n   <div style=\"text-align: center;\">\n   <span style=\"font-size: 24px;\">  $[\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}]$ </span>\n   </div>\n  Here, accuracy is calculated as $\\frac{(61+25+104)}{227} = 0.837$.  \n\n#### **Precision**\n- **What it means:** Precision measures the accuracy of positive predictions made by the model. It is the ratio of correctly predicted positive observations to the total predicted positives. Precision is crucial when the cost of false positives is high, as it indicates how many of the positive predictions made by the model were actually correct.\n- **Calculation:**\n  <div style=\"text-align: center;\">\n   <span style=\"font-size: 24px;\">\n   $\\text{Precision} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Positives (FP)}}$\n   </span>\n</div>\n\n- **Example:** Precision for the positive class is 0.71, meaning 71% of the instances that were predicted as positive by the model were actually positive.\n\n#### **Recall**\n- **What it means:** Recall measures the ability of the model to identify all relevant instances of a class. It is the ratio of correctly predicted positive observations to all observations in the actual class.\n- **Calculation:** \n  <div style=\"text-align: center;\">\n   <span style=\"font-size: 24px;\"> \n   $\\text{Recall} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Negatives (FN)}}$\n   </span>\n  </div>\n\n- **Example:** Recall for the positive class is 0.92, meaning 92% of actual positive instances were correctly identified by the model.\n\n#### **F1-Score**\n- **What it means:** F1-Score is the harmonic mean of Precision and Recall. It provides a balance between precision and recall, especially useful when you want to find an equilibrium between these two metrics.\n- **Calculation:**\n  <div style=\"text-align: center;\">\n   <span style=\"font-size: 24px;\">\n   $\\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$\n   </span><br><br>\n</div>\n- **Example:** The F1-Score for the positive class is 0.80, indicating a balanced performance between precision and recall for this class.\n\n\n### **Confusion Matrix** (May change, try to get the general idea)\n| Actual \\ Predicted | Positive | Negative | Neutral |\n|--------------------|----------|----------|---------|\n| **Positive**       |    61    |    1     |    4    |\n| **Negative**       |    0     |    25    |    0    |\n| **Neutral**        |    25    |    7     |    104  |\n  \n\n\n- **[61, 1, 4]:** \n  - **61:** True positive (correctly predicted positive).\n  - **1:** False negative (predicted negative but actually positive).\n  - **4:** False positive (predicted neutral but actually positive).\n  \n- **[0, 25, 0]:** \n  - **25:** True negative (correctly predicted negative).\n  - **0:** No instances of incorrect prediction in this class.\n\n- **[25, 7, 104]:** \n  - **24:** False negative (predicted positive but actually neutral).\n  - **7:** False positive (predicted negative but actually neutral).\n  - **104:** True neutral (correctly predicted neutral).\n  \n","metadata":{}},{"cell_type":"markdown","source":"# Identifying 4-Bit Quantized Layers for Fine-Tuning\n\n- **Purpose**: Optimize fine-tuning of large models by identifying and targeting specific 4-bit quantized layers for advanced techniques like LoRA (Low-Rank Adaptation).\n\n- **Quantized Layers**: The focus is on layers that have been quantized to 4-bit precision using the `bitsandbytes` library.\n\n- **Efficiency**: Targeting only the identified 4-bit quantized linear layers allows for a more resource-efficient fine-tuning process.\n\n- **Performance Balance**: By selectively applying fine-tuning techniques to these layers, we achieve a balance between enhancing model performance and maintaining computational efficiency.\n\n- **Advanced Fine-Tuning**: This approach enables the application of sophisticated fine-tuning methods, specifically designed for large models, to improve their adaptability to specific tasks.\n\n### Explanation\n\n- **`cls = bnb.nn.Linear4bit`**: \n  - Refers to the `Linear4bit` class from the `bitsandbytes` library, which is used to identify 4-bit quantized layers in the model.\n\n- **`lora_module_names = set()`**: \n  - Initializes a set to store unique names of the 4-bit quantized layers that will be targeted for LoRA fine-tuning.\n\n- **`if isinstance(module, cls):`**: \n  - Checks if the current module is an instance of the `Linear4bit` class, meaning it is a 4-bit quantized layer suitable for LoRA fine-tuning.\n\n- **`lora_module_names.add(names[0] if len(names) == 1 else names[-1])`**: \n  - Adds the name of the identified 4-bit quantized layer to the set, focusing on the last part of the name if it is nested.\n\n- **`if 'lm_head' in lora_module_names:`**:\n  - Checks if `'lm_head'` (usually the output layer) is in the set, often excluded from LoRA fine-tuning.\n\n- **`lora_module_names.remove('lm_head')`**:\n  - Removes `'lm_head'` from the set to prevent it from being fine-tuned.\n\n- **`return list(lora_module_names)`**:\n  - Returns a list of the names of 4-bit quantized layers that are suitable for LoRA fine-tuning.\n\n\n","metadata":{}},{"cell_type":"code","source":"def find_all_linear_names(model):\n    cls = bnb.nn.Linear4bit \n    lora_module_names = set()\n    \n    for name, module in model.named_modules():\n        if isinstance(module, cls): \n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n    \n    # If 'lm_head' is in the set, remove it (often needed for 16-bit models)\n    if 'lm_head' in lora_module_names:  \n        lora_module_names.remove('lm_head')\n    \n    return list(lora_module_names)\n\nmodules = find_all_linear_names(model)\nmodules","metadata":{"execution":{"iopub.status.busy":"2024-08-20T06:22:54.631501Z","iopub.execute_input":"2024-08-20T06:22:54.632528Z","iopub.status.idle":"2024-08-20T06:22:54.649986Z","shell.execute_reply.started":"2024-08-20T06:22:54.632493Z","shell.execute_reply":"2024-08-20T06:22:54.647474Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"['q_proj', 'down_proj', 'o_proj', 'gate_proj', 'up_proj', 'v_proj', 'k_proj']"},"metadata":{}}]},{"cell_type":"markdown","source":"# Training Settings\n\n### `peft_config = LoraConfig(...)`\n\n- **`lora_alpha=16`**: \n  - Controls the scaling factor for LoRA (Low-Rank Adaptation) layers, affecting how much the fine-tuning layers influence the overall model.\n\n- **`lora_dropout=0`**: \n  - Specifies that no dropout will be applied in LoRA layers, meaning that all neurons will be used during each forward pass.\n\n- **`r=64`**: \n  - Defines the rank of the LoRA layers, which determines the dimensionality of the low-rank matrices. A higher rank can capture more complex interactions but requires more memory.\n\n- **`bias=\"none\"`**: \n  - Indicates that no bias term will be added in the LoRA layers, simplifying the adaptation.\n\n- **`task_type=\"CAUSAL_LM\"`**: \n  - Specifies that the task is Causal Language Modeling, where the model predicts the next token in a sequence based on previous tokens.\n\n- **`target_modules=modules`**: \n  - Assigns the LoRA layers to the specific modules identified using `find_all_linear_names`, ensuring that only certain parts of the model are fine-tuned.\n  \n### `training_arguments = TrainingArguments(...)`\n\n- **`num_train_epochs=1`**: \n  - Indicates that the model will be trained for 1 epoch, meaning it will pass through the entire training dataset once.\n\n- **`per_device_train_batch_size=1`**: \n  - Sets the batch size to 1 per device during training, meaning each GPU or TPU will process one example at a time.\n\n- **`gradient_accumulation_steps=8`**: \n  - Accumulates gradients over 8 steps before performing a backward pass and model update. This effectively increases the batch size without requiring more memory.\n\n- **`optim=\"paged_adamw_32bit\"`**: \n  - Uses the 32-bit AdamW optimizer with paged memory management, which is more memory-efficient and suitable for fine-tuning large models.\n\n- **`logging_steps=1`**: \n  - Logs training metrics after every step, allowing for detailed tracking of the training process.\n\n- **`learning_rate=2e-4`**: \n  - Sets the learning rate to 0.0002, which is based on the QLoRA paper. This controls how quickly the model updates its parameters during training.\n\n- **`weight_decay=0.001`**: \n  - Applies a weight decay of 0.001, which is a regularization technique to prevent the model from overfitting by penalizing large weights.\n\n- **`max_grad_norm=0.3`**: \n  - Clips gradients to a maximum norm of 0.3, which helps in stabilizing training and preventing gradient explosions.\n\n- **`warmup_ratio=0.03`**: \n  - Uses a warmup ratio of 0.03, meaning 3% of the total training steps are used for a gradual learning rate increase from 0 to the specified rate, following the QLoRA paper.\n\n- **`group_by_length=False`**: \n  - Disables grouping of examples by their length, meaning that examples will not be sorted or batched based on their sequence length.\n\n- **`lr_scheduler_type=\"cosine\"`**: \n  - Uses a cosine learning rate scheduler, which gradually decreases the learning rate following a cosine curve.\n\n- **`eval_strategy=\"steps\"`**: \n  - Specifies that evaluation will be performed at regular steps during training rather than after each epoch.\n\n- **`eval_steps=int(0.2 * len(train_data))`**: \n  - Sets the number of steps between evaluations to 20% of the training data length, ensuring frequent model evaluation during training.\n\n---\n\n### `trainer = SFTTrainer(...)`\n\n- **`train_dataset=train_data`**: \n  - The dataset used for training the model, containing the processed training examples.\n\n- **`eval_dataset=eval_data`**: \n  - The dataset used for evaluation during training, allowing for validation of the model's performance.\n\n- **`dataset_text_field=\"text\"`**: \n  - Indicates that the dataset field containing the text data is labeled \"text.\"\n\n- **`max_seq_length=512`**: \n  - Sets the maximum sequence length for padding and truncation, ensuring all inputs are of uniform length.\n\n- **`packing=False`**: \n  - Disables packing of multiple examples into one input sequence, meaning each input will be processed individually.\n\n- **`dataset_kwargs={\"add_special_tokens\": False, \"append_concat_token\": False}`**: \n  - Additional settings for handling the dataset, where special tokens are not added, and no concatenation token is appended.\n","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig\noutput_dir=\"llama-3.1-fine-tuned-model\"\n\npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0,\n    r=64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=modules,  # Retaining the modules found using find_all_linear_names\n)\n\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,                    # Directory to save the model\n    num_train_epochs=1,                       # Number of training epochs\n    per_device_train_batch_size=1,            # Batch size per device during training\n    gradient_accumulation_steps=8,            # Number of steps before performing a backward/update pass\n    gradient_checkpointing=True,              # Use gradient checkpointing to save memory\n    optim=\"paged_adamw_32bit\",                # Optimizer set for 32-bit AdamW (efficient memory usage)\n    logging_steps=1,                         \n    learning_rate=2e-4,                       # Learning rate, based on QLoRA paper (https://arxiv.org/abs/2305.14314 )\n    weight_decay=0.001,\n    fp16=True,                                # Using fp16 as the model was loaded in fp16 (float16) precision\n    bf16=False,                               \n    max_grad_norm=0.3,                        # Max gradient norm based on QLoRA paper\n    max_steps=-1,\n    warmup_ratio=0.03,                        # Warmup ratio based on QLoRA paper\n    group_by_length=False,\n    lr_scheduler_type=\"cosine\",               # Cosine learning rate scheduler\n    report_to=\"wandb\",                        # Report metrics to W&B\n    eval_strategy=\"steps\",                    # Save checkpoint every few steps rather than epochs\n    eval_steps=int(0.2 * len(train_data)),    # Eval steps based on a portion of the training data\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    args=training_arguments,\n    train_dataset=train_data,\n    eval_dataset=eval_data,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",               # The field name in the dataset containing the text\n    tokenizer=tokenizer,\n    max_seq_length=512,                      # Max sequence length for padding/truncation\n    packing=False,                           # No packing of multiple examples into one input sequence\n    dataset_kwargs={\n        \"add_special_tokens\": False,         # Do not add special tokens, based on your previous settings\n        \"append_concat_token\": False,        # Do not append concatenation token\n    }\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T06:22:56.050418Z","iopub.execute_input":"2024-08-20T06:22:56.051132Z","iopub.status.idle":"2024-08-20T06:22:59.689714Z","shell.execute_reply.started":"2024-08-20T06:22:56.0511Z","shell.execute_reply":"2024-08-20T06:22:59.688555Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length, dataset_kwargs. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:366: UserWarning: You passed a `dataset_kwargs` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1811 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0116f5344b1c4eca860eb80c7e10e8b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/226 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d1dc8812dbd4266a8756fd013910f4a"}},"metadata":{}}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-08-20T06:23:10.329209Z","iopub.execute_input":"2024-08-20T06:23:10.329913Z","iopub.status.idle":"2024-08-20T06:55:31.892281Z","shell.execute_reply.started":"2024-08-20T06:23:10.329879Z","shell.execute_reply":"2024-08-20T06:55:31.891354Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='226' max='226' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [226/226 32:12, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=226, training_loss=1.5630741752354445, metrics={'train_runtime': 1940.9691, 'train_samples_per_second': 0.933, 'train_steps_per_second': 0.116, 'total_flos': 3913213492862976.0, 'train_loss': 1.5630741752354445, 'epoch': 0.9983434566537824})"},"metadata":{}}]},{"cell_type":"code","source":"wandb.finish()\nmodel.config.use_cache = True","metadata":{"execution":{"iopub.status.busy":"2024-08-20T07:13:21.850816Z","iopub.execute_input":"2024-08-20T07:13:21.851227Z","iopub.status.idle":"2024-08-20T07:13:27.82739Z","shell.execute_reply.started":"2024-08-20T07:13:21.851194Z","shell.execute_reply":"2024-08-20T07:13:27.826673Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27a47441c5544aafa19e5381124f5024"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▂▂█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▃▇██████▇▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▅▄▃▁▂▃▂▂▂▃▁▂▁▂▂▁▁▂▁▁▂▂▂▂▂▁▂▂▂▁▁▁▂▂▂▂▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>3913213492862976.0</td></tr><tr><td>train/epoch</td><td>0.99834</td></tr><tr><td>train/global_step</td><td>226</td></tr><tr><td>train/grad_norm</td><td>0.40368</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.7057</td></tr><tr><td>train_loss</td><td>1.56307</td></tr><tr><td>train_runtime</td><td>1940.9691</td></tr><tr><td>train_samples_per_second</td><td>0.933</td></tr><tr><td>train_steps_per_second</td><td>0.116</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">dutiful-hill-8</strong> at: <a href='https://wandb.ai/kajuyerim-bo-azi-i-niversitesi/Fine%20tuning%20LLama3.1%208b%20for%20Sentimental%20Analysis%20on%20Financial%20Domain/runs/3wqscyc9' target=\"_blank\">https://wandb.ai/kajuyerim-bo-azi-i-niversitesi/Fine%20tuning%20LLama3.1%208b%20for%20Sentimental%20Analysis%20on%20Financial%20Domain/runs/3wqscyc9</a><br/> View project at: <a href='https://wandb.ai/kajuyerim-bo-azi-i-niversitesi/Fine%20tuning%20LLama3.1%208b%20for%20Sentimental%20Analysis%20on%20Financial%20Domain' target=\"_blank\">https://wandb.ai/kajuyerim-bo-azi-i-niversitesi/Fine%20tuning%20LLama3.1%208b%20for%20Sentimental%20Analysis%20on%20Financial%20Domain</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240820_061739-3wqscyc9/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."},"metadata":{}}]},{"cell_type":"code","source":"# Save trained model and tokenizer\ntrainer.save_model(output_dir)\ntokenizer.save_pretrained(output_dir)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T07:13:34.081674Z","iopub.execute_input":"2024-08-20T07:13:34.082047Z","iopub.status.idle":"2024-08-20T07:13:35.905522Z","shell.execute_reply.started":"2024-08-20T07:13:34.082014Z","shell.execute_reply":"2024-08-20T07:13:35.904464Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"('llama-3.1-fine-tuned-model/tokenizer_config.json',\n 'llama-3.1-fine-tuned-model/special_tokens_map.json',\n 'llama-3.1-fine-tuned-model/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"y_pred = predict(X_test, model, tokenizer)\nevaluate(y_true, y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T07:13:38.303185Z","iopub.execute_input":"2024-08-20T07:13:38.303842Z","iopub.status.idle":"2024-08-20T07:16:04.146834Z","shell.execute_reply.started":"2024-08-20T07:13:38.303806Z","shell.execute_reply":"2024-08-20T07:16:04.145773Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"  0%|          | 0/227 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n100%|██████████| 227/227 [02:25<00:00,  1.56it/s]","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.952\nAccuracy for label positive: 0.909\nAccuracy for label negative: 0.960\nAccuracy for label neutral: 0.971\n\nClassification Report:\n              precision    recall  f1-score   support\n\n    positive       0.94      0.91      0.92        66\n    negative       1.00      0.96      0.98        25\n     neutral       0.95      0.97      0.96       136\n\n    accuracy                           0.95       227\n   macro avg       0.96      0.95      0.95       227\nweighted avg       0.95      0.95      0.95       227\n\n\nConfusion Matrix:\n[[ 60   0   6]\n [  0  24   1]\n [  4   0 132]]\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Model Performance Comparison\n\n### Accuracy\n- **Before Fine-Tuning with 20% of the Dataset:**\n  - Accuracy: 0.837\n  - Accuracy for label positive: 0.924\n  - Accuracy for label negative: 1.000\n  - Accuracy for label neutral: 0.765\n\n- **After Fine-Tuning:**\n  - Accuracy: 0.952\n  - Accuracy for label positive: 0.909\n  - Accuracy for label negative: 0.960\n  - Accuracy for label neutral: 0.971\n\n### Precision, Recall, F1-Score Comparison\n\n| Class      | Precision (Before) | Precision (After) | Recall (Before) | Recall (After) | F1-Score (Before) | F1-Score (After) |\n|------------|--------------------|-------------------|-----------------|----------------|-------------------|------------------|\n| Positive   | 0.71               | 0.94              | 0.92            | 0.91           | 0.80              | 0.92             |\n| Negative   | 0.76               | 1.00              | 1.00            | 0.96           | 0.86              | 0.98             |\n| Neutral    | 0.96               | 0.95              | 0.76            | 0.97           | 0.85              | 0.96             |\n\n| **Overall** | **Precision (Before)** | **Precision (After)** | **Recall (Before)** | **Recall (After)** | **F1-Score (Before)** | **F1-Score (After)** |\n|-------------|------------------------|-----------------------|---------------------|--------------------|-----------------------|----------------------|\n| **Macro avg** | 0.81                 | 0.96                  | 0.90                | 0.95               | 0.84                  | 0.95                 |\n| **Weighted avg** | 0.87              | 0.95                  | 0.84                | 0.95               | 0.84                  | 0.95                 |\n\n","metadata":{}}]}