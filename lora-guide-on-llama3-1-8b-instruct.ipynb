{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":81881,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":68809,"modelId":91102}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/kajuyerim/lora-guide-on-llama3-1-8b-instruct?scriptVersionId=191364321\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# LoRA Guide About Sentimental Analysis on Financial Domain Using Llama3.1 8b-instruct\n---\n\n<font size=\"5\">**What is LoRA?**</font> \n* #### Low Rank Adaptation: Freezing the pretrained model weights and injecting trainable rank decomposition matrices into each layer of the Transformer architecture.\n* #### Main purpose is to lower the dimensions of the matrix by \n* #### Instead of updating all weights of a model, we only update the injected low rank matrices.\n\n<font size=\"5\">**Why Use LoRA?**</font> \n* #### Greatly reducing the number of trainable parameters (up to 10000 times).\n* #### Reducing the GPU memory requirement (up to 3 times).\n\n---\n<font size=\"4\">**! This guide is based on the LoRA paper (LoRA: Low-Rank Adaptation for Neural Networks)** (https://arxiv.org/abs/2106.09685#).</font>   \n<font size=\"4\">**! Any mistakes between this guide and the paper are due to my interpretation on the material.**</font>  \n---\n","metadata":{}},{"cell_type":"code","source":"!pip install -U transformers datasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-05T10:33:08.414083Z","iopub.execute_input":"2024-08-05T10:33:08.414721Z","iopub.status.idle":"2024-08-05T10:33:33.027596Z","shell.execute_reply.started":"2024-08-05T10:33:08.414689Z","shell.execute_reply":"2024-08-05T10:33:33.02653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We need to prepare the dataset and load it so that we can use it to train our model with LoRA.","metadata":{}},{"cell_type":"code","source":"# Re-import necessary libraries since the code execution state was reset.\nfrom datasets import load_dataset\n\n# Load and shuffle the dataset with a specified configuration\ndataset = load_dataset(\"takala/financial_phrasebank\", \"sentences_allagree\", split='train')\nshuffled_dataset = dataset.shuffle(seed=50)\n\n# Map numerical labels to string labels\nlabel_mapping = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n\n# Format examples into \"sentence\" - \"sentiment\" format\nformatted_examples = [\n    f'\"{example[\"sentence\"]}\" - \"{label_mapping[example[\"label\"]]}\"' for example in shuffled_dataset\n]\n\n# Print the first 5 formatted examples\nfor example in formatted_examples[:10]:\n    print(example)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-05T10:33:38.446844Z","iopub.execute_input":"2024-08-05T10:33:38.447218Z","iopub.status.idle":"2024-08-05T10:33:44.65943Z","shell.execute_reply.started":"2024-08-05T10:33:38.447189Z","shell.execute_reply":"2024-08-05T10:33:44.658471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n\nlogin(token = hf_token)","metadata":{"execution":{"iopub.status.busy":"2024-08-05T10:33:48.503051Z","iopub.execute_input":"2024-08-05T10:33:48.503424Z","iopub.status.idle":"2024-08-05T10:33:48.941709Z","shell.execute_reply.started":"2024-08-05T10:33:48.503393Z","shell.execute_reply":"2024-08-05T10:33:48.940788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer,AutoModelForCausalLM,pipeline\nimport torch\n\nbase_model = \"/kaggle/input/llama-3.1/transformers/8b-instruct/1\"\n\ntokenizer = AutoTokenizer.from_pretrained(base_model)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n        base_model,\n        return_dict=True,\n        low_cpu_mem_usage=True,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n        trust_remote_code=True,\n)\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-05T10:33:51.447337Z","iopub.execute_input":"2024-08-05T10:33:51.447697Z","iopub.status.idle":"2024-08-05T10:33:52.094378Z","shell.execute_reply.started":"2024-08-05T10:33:51.44767Z","shell.execute_reply":"2024-08-05T10:33:52.093098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}